{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONRxYIGDHNr7"
      },
      "source": [
        "# Deep Memory trained on Syntethic Queries improves recall@10 by +20%\n",
        "\n",
        "You need to have labelled data (query and relevance pairs) for training deep memory. However it is sometimes hard to obtain labelled data when you start fresh.\n",
        "\n",
        "In this tutorial we will take an existing dataset and generate queries using GPT to train Deep Memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayEgfW2vHPDK"
      },
      "source": [
        "## 0. Setup packages and credentials\n",
        "Install Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCZJ5PXlGMu6"
      },
      "outputs": [],
      "source": [
        "!pip3 install deeplake langchain openai tiktoken llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRYFBOBMHG16"
      },
      "source": [
        "Setup Activeloop and OpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re7qIcjoGrdm"
      },
      "outputs": [],
      "source": [
        "import os, getpass\n",
        "os.environ['ACTIVELOOP_TOKEN'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl6ceqfwGyS0"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass.getpass()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVACLdPZHqwW"
      },
      "source": [
        "## 1. Load the dataset and create a Deep Lake vector store\n",
        "\n",
        "We are going to use GPT3.5 to generate questions based on the context provided by a chunk test."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p 'data/paul_graham/'\n",
        "!curl 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/examples/data/paul_graham/paul_graham_essay.txt' -o 'data/paul_graham/paul_graham_essay.txt'"
      ],
      "metadata": {
        "id": "-NCMGoopuG0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "from llama_index import SimpleDirectoryReader\n",
        "\n",
        "documents = SimpleDirectoryReader(\"./data/paul_graham/\").load_data()\n",
        "node_parser = SimpleNodeParser.from_defaults(chunk_size=512)\n",
        "nodes = node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# By default, the node/chunks ids are set to random uuids. To ensure same id's per run, we manually set them.\n",
        "for idx, node in enumerate(nodes):\n",
        "    node.id_ = f\"node_{idx}\"\n",
        "\n",
        "print(f\"Number of Documents: {len(documents)}\")\n",
        "print(f\"Number of nodes: {len(nodes)} with the current chunk size of {node_parser.chunk_size}\")"
      ],
      "metadata": {
        "id": "h2Q8HcqxuJiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNbSMwL8OY_H"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex, ServiceContext, StorageContext\n",
        "from llama_index.vector_stores import DeepLakeVectorStore\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "# Create a DeepLakeVectorStore locally to store the vectors\n",
        "dataset_path = \"./data/paul_graham/deep_lake_db\"\n",
        "vector_store = DeepLakeVectorStore(dataset_path=dataset_path, overwrite=True)\n",
        "\n",
        "# LLM that will answer questions with the retrieved context\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "vector_index = VectorStoreIndex(nodes, service_context=service_context, storage_context=storage_context, show_progress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's upload the local Vectore Store to Active Loop's platform and then convert it into a managed database."
      ],
      "metadata": {
        "id": "HKiyo5SpuxYD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yW2imseOuLG"
      },
      "outputs": [],
      "source": [
        "import deeplake\n",
        "local = \"./data/paul_graham/deep_lake_db\"\n",
        "hub_path = \"hub://genai360/optimization_paul_graham\"\n",
        "hub_managed_path = \"hub://genai360/optimization_paul_graham_managed\"\n",
        "\n",
        "# First upload our local vector store\n",
        "deeplake.deepcopy(local, hub_path, overwrite=True)\n",
        "# Create a managed vector store under a different name\n",
        "deeplake.deepcopy(hub_path, hub_managed_path, overwrite=True, runtime={\"tensor_db\": True})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6A-WROoEOz38"
      },
      "source": [
        "## 2. Generate a dataset of Queries and Documents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fetch dataset docs and ids if they exist (optional you can also ingest)\n",
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)\n",
        "docs = db.vectorstore.dataset.text.data(fetch_chunks=True, aslist=True)['value']\n",
        "ids = db.vectorstore.dataset.id.data(fetch_chunks=True, aslist=True)['value']\n",
        "print(len(docs))"
      ],
      "metadata": {
        "id": "t_1d6IRqvvUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7pjHtslc9Ao"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "def generate_question(text):\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a world class expert for generating questions based on provided context. \\\n",
        "                        You make sure the question can be answered by the text.\"},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": text,\n",
        "                },\n",
        "            ],\n",
        "        )\n",
        "        return response.choices[0].message.content\n",
        "    except:\n",
        "        question_string = \"No question generated\"\n",
        "        return question_string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "coaUWAp4ISyw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_queries(docs: list[str], ids: list[str], n: int):\n",
        "\n",
        "    questions = []\n",
        "    relevances = []\n",
        "    pbar = tqdm(total=n)\n",
        "    while len(questions) < n:\n",
        "        # 1. randomly draw a piece of text and relevance id\n",
        "        r = random.randint(0, len(docs)-1)\n",
        "        text, label = docs[r], ids[r]\n",
        "\n",
        "        # 2. generate queries and assign and relevance id\n",
        "        generated_qs = [generate_question(text)]\n",
        "        if generated_qs == [\"No question generated\"]:\n",
        "            print(\"No question generated\")\n",
        "            continue\n",
        "\n",
        "        questions.extend(generated_qs)\n",
        "        relevances.extend([[(label, 1)] for _ in generated_qs])\n",
        "        pbar.update(len(generated_qs))\n",
        "\n",
        "    return questions[:n], relevances[:n]\n",
        "\n",
        "# Here we choose to generate 40 questions\n",
        "questions, relevances = generate_queries(docs, ids, n=40)\n",
        "print(len(questions))\n",
        "print(questions[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKETptufMlDb"
      },
      "source": [
        "## 3. Train Deep Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qhx2PjztMmdf"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "\n",
        "openai_embeddings = OpenAIEmbeddings()\n",
        "\n",
        "job_id = db.vectorstore.deep_memory.train(\n",
        "    queries=questions,\n",
        "    relevance=relevances,\n",
        "    embedding_function=openai_embeddings.embed_documents,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XneeHJdOE0g"
      },
      "outputs": [],
      "source": [
        "db.vectorstore.deep_memory.status('65a19020be524eca0ecb9a6e')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFoi25nXXj4L"
      },
      "source": [
        "Wait until training status becomes completed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znNn87saMnr1"
      },
      "source": [
        "## 4. Evaluate Deep Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37vIbzAQSGD9"
      },
      "source": [
        "### 4.1 Manual"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgi-zdwuSBNj"
      },
      "outputs": [],
      "source": [
        "from llama_index.llms import OpenAI\n",
        "query = \"What are the main things Paul worked on before college?\"\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo-1106\")\n",
        "embed_model = OpenAIEmbedding()\n",
        "\n",
        "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm,)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "db = DeepLakeVectorStore(dataset_path=hub_managed_path, overwrite=False, read_only=True,)\n",
        "vector_index = VectorStoreIndex.from_vector_store(db, service_context=service_context, storage_context=storage_context, show_progress=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53ht4F2fR97p"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": False})\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxlN0VrER-ap"
      },
      "outputs": [],
      "source": [
        "query_engine = vector_index.as_query_engine(similarity_top_k=3, vector_store_kwargs={\"deep_memory\": True})\n",
        "response_vector = query_engine.query(query)\n",
        "print(response_vector.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZBoA8YqSI4b"
      },
      "source": [
        "### 4.2 Quantitative Evaluation on Synthetically generated queries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd65U190Mp9v"
      },
      "outputs": [],
      "source": [
        "validation_questions, validation_relevances = generate_queries(docs, ids, n=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXJYCiwNPbOL"
      },
      "outputs": [],
      "source": [
        "recalls = db.vectorstore.deep_memory.evaluate(\n",
        "    queries=validation_questions,\n",
        "    relevance=validation_relevances,\n",
        "    embedding_function=openai_embeddings.embed_documents,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}