{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5789bc3-b1ae-42c7-94a8-2ef4f89946fc",
   "metadata": {},
   "source": [
    "# Lesson 4: Persistence and Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314bce41",
   "metadata": {},
   "source": [
    "Persistence and streaming\n",
    "\n",
    "Persistence lets you keep around the state\n",
    "of an agent at a particular point in time.\n",
    "This can let you go back to that state and resume in that state\n",
    "in future interactions.\n",
    "This is really important for long running applications.\n",
    "\n",
    "Likewise, with streaming,\n",
    "you can emit a list of signals of what's going on at that exact moment.\n",
    "So for long running applications, you know exactly what the agent is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b5acb3",
   "metadata": {},
   "source": [
    "we've added the concept of a check pointer into LangGraph.\n",
    "A check pointer basically checkpoints\n",
    "the state after\n",
    "and between every node.\n",
    "To add in persistence for this agent,\n",
    "what we'll do is we'll use a SqliteSaver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5762271-8736-4e94-9444-8c92bd0e8074",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0168aee-bce9-4d60-b827-f86a88187e31",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da06a64f-a2d5-4a66-8090-9ada0930c684",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "tool = TavilySearchResults(max_results=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2589c5b6-6cc2-4594-9a17-dccdcf676054",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c033522-d2fc-41ac-8e3c-5e35872bf88d",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "memory = SqliteSaver.from_conn_string(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee8ef4c",
   "metadata": {},
   "source": [
    "In order to deal with persistence,\n",
    "we've added the concept of a check pointer into LangGraph.\n",
    "A check pointer basically checkpoints\n",
    "the state after\n",
    "and between every node.\n",
    "To add in persistence for this agent,\n",
    "what we'll do is we'll use a SqliteSaver.\n",
    "So this is a really simple check pointer that we've added that uses\n",
    "Sqlite, a built in database under the hood.\n",
    "And we'll just use the in-memory database.\n",
    "So if we refresh this notebook it'll disappear.\n",
    "But you can easily connect this to an external database\n",
    "or we also have other check pointers that use Redis\n",
    "and Postgres and other more persistent databases like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ba84ec-c172-4de7-ac55-e3158a531b23",
   "metadata": {
    "height": 574
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system=\"\"):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "876d5092-b8ef-4e38-b4d7-0e80c609bf7a",
   "metadata": {
    "height": 132
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121309c3",
   "metadata": {},
   "source": [
    "**Streaming**\n",
    "\n",
    "First, we might care about streaming\n",
    "the individual messages.\n",
    "So this would be the Al message that determines what action to take.\n",
    "And then the observation message that represents the result\n",
    "of taking that action.\n",
    "The second thing we might care about streaming is tokens.\n",
    "So for each token of the LLM call we might want to stream the output.\n",
    "\n",
    "We're now going to add this concept of a thread config.\n",
    "So this will be used to keep track of different threads\n",
    "inside the persistent checkpointer.\n",
    "This will allow us to have multiple conversations going on at the same time.\n",
    "\n",
    "This is really needed for production applications\n",
    "where you generally have many users.\n",
    "This thread config is simply a dictionary with a configurable key.\n",
    "And as part of that, we have a thread id and we can set that equal to any string.\n",
    "Here we're going to set that equal to one.\n",
    "\n",
    "We're now going to call the graph\n",
    "not with invoke, but with stream.\n",
    "We're going to pass in the same messages dictionary.\n",
    "And we're also going to pass in this thread config as a second parameter there.\n",
    "We're then going to get back a stream of events.\n",
    "These events represent updates to that state over time\n",
    "because we know our state only has one key,\n",
    "the messages key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e8d938",
   "metadata": {},
   "source": [
    "Each thread id represents one session or similar to one chat_id, langgraph will store all pvs messages w.r.t that thread_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10084a02-2928-4945-9f7c-ad3f5b33caf7",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in sf?\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "714d1205-f8fc-4912-b148-2a45da99219c",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83588e70-254f-4f83-a510-c8ae81e729b0",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mTnplhSZin0eHzj4Jk4nTUpW', 'function': {'arguments': '{\"query\":\"current weather in San Francisco\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 151, 'total_tokens': 173, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-2fbd26d8-24d4-43ba-af70-6782c6ecc00d-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_mTnplhSZin0eHzj4Jk4nTUpW'}])]\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_mTnplhSZin0eHzj4Jk4nTUpW'}\n",
      "Back to the model!\n",
      "[ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'San Francisco\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 37.78, \\'lon\\': -122.42, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1726282576, \\'localtime\\': \\'2024-09-13 19:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1726281900, \\'last_updated\\': \\'2024-09-13 19:45\\', \\'temp_c\\': 18.9, \\'temp_f\\': 66.0, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Partly cloudy\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/116.png\\', \\'code\\': 1003}, \\'wind_mph\\': 8.9, \\'wind_kph\\': 14.4, \\'wind_degree\\': 257, \\'wind_dir\\': \\'WSW\\', \\'pressure_mb\\': 1009.0, \\'pressure_in\\': 29.79, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 78, \\'cloud\\': 25, \\'feelslike_c\\': 18.9, \\'feelslike_f\\': 66.0, \\'windchill_c\\': 14.8, \\'windchill_f\\': 58.7, \\'heatindex_c\\': 15.2, \\'heatindex_f\\': 59.4, \\'dewpoint_c\\': 11.7, \\'dewpoint_f\\': 53.0, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 5.0, \\'gust_mph\\': 15.3, \\'gust_kph\\': 24.7}}\"}, {\\'url\\': \\'https://www.timeanddate.com/weather/@z-us-94124/ext\\', \\'content\\': \\'San Francisco 14 Day Extended Forecast. Weather Today Weather Hourly 14 Day Forecast Yesterday/Past Weather Climate (Averages) Currently: 62 °F. Passing clouds. (Weather station: San Francisco International Airport, USA). See more current weather.\\'}]', name='tavily_search_results_json', tool_call_id='call_mTnplhSZin0eHzj4Jk4nTUpW')]\n",
      "[AIMessage(content='The current weather in San Francisco is partly cloudy with a temperature of 18.9°C (66.0°F). The wind is blowing from the west-southwest at 8.9 mph (14.4 kph). The humidity level is at 78%, and visibility is 16 km (9 miles).', response_metadata={'token_usage': {'completion_tokens': 66, 'prompt_tokens': 667, 'total_tokens': 733, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-ee7773b7-f916-4344-8d85-1bedd898af64-0')]\n"
     ]
    }
   ],
   "source": [
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cb3ef4c-58b3-401b-b104-0d51e553d982",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_bybXck117IDFCptjw9YDQ5Wn', 'function': {'arguments': '{\"query\":\"current weather in Los Angeles\"}', 'name': 'tavily_search_results_json'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 745, 'total_tokens': 767, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-c61ea791-c9dd-4a86-8c25-805b9bbedb5a-0', tool_calls=[{'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_bybXck117IDFCptjw9YDQ5Wn'}])]}\n",
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in Los Angeles'}, 'id': 'call_bybXck117IDFCptjw9YDQ5Wn'}\n",
      "Back to the model!\n",
      "{'messages': [ToolMessage(content='[{\\'url\\': \\'https://www.weatherapi.com/\\', \\'content\\': \"{\\'location\\': {\\'name\\': \\'Los Angeles\\', \\'region\\': \\'California\\', \\'country\\': \\'United States of America\\', \\'lat\\': 34.05, \\'lon\\': -118.24, \\'tz_id\\': \\'America/Los_Angeles\\', \\'localtime_epoch\\': 1726282580, \\'localtime\\': \\'2024-09-13 19:56\\'}, \\'current\\': {\\'last_updated_epoch\\': 1726281900, \\'last_updated\\': \\'2024-09-13 19:45\\', \\'temp_c\\': 21.7, \\'temp_f\\': 71.1, \\'is_day\\': 0, \\'condition\\': {\\'text\\': \\'Clear\\', \\'icon\\': \\'//cdn.weatherapi.com/weather/64x64/night/113.png\\', \\'code\\': 1000}, \\'wind_mph\\': 6.0, \\'wind_kph\\': 9.7, \\'wind_degree\\': 213, \\'wind_dir\\': \\'SSW\\', \\'pressure_mb\\': 1010.0, \\'pressure_in\\': 29.81, \\'precip_mm\\': 0.0, \\'precip_in\\': 0.0, \\'humidity\\': 73, \\'cloud\\': 0, \\'feelslike_c\\': 21.7, \\'feelslike_f\\': 71.1, \\'windchill_c\\': 22.8, \\'windchill_f\\': 73.1, \\'heatindex_c\\': 24.9, \\'heatindex_f\\': 76.8, \\'dewpoint_c\\': 13.8, \\'dewpoint_f\\': 56.9, \\'vis_km\\': 16.0, \\'vis_miles\\': 9.0, \\'uv\\': 6.0, \\'gust_mph\\': 7.2, \\'gust_kph\\': 11.5}}\"}, {\\'url\\': \\'https://forecast.weather.gov/MapClick.php?site=LOX&textField1=34.0522&textField2=-118.243&e=0\\', \\'content\\': \\'Current conditions at LOS ANGELES DOWNTOWN (FHMC1) Lat: 34.06778°NLon: 118.24167°WElev: 413.0ft. NA. 66°F. 19°C. ... 09 pm PDT Sep 12, 2024. Forecast Valid: 1am PDT Sep 13, 2024-6pm PDT Sep 19, 2024 . ... Severe Weather ; Current Outlook Maps ; Drought ; Fire Weather ;\\'}]', name='tavily_search_results_json', tool_call_id='call_bybXck117IDFCptjw9YDQ5Wn')]}\n",
      "{'messages': [AIMessage(content='The current weather in Los Angeles is clear with a temperature of 21.7°C (71.1°F). The wind is coming from the south-southwest at 6.0 mph (9.7 kph). The humidity level is at 73%, and visibility is 16 km (9 miles).', response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1330, 'total_tokens': 1395, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-bb8ab984-c313-4d2c-8dbb-dd93ab19c306-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What about in la?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7d1b81",
   "metadata": {},
   "source": [
    "Using persistence and streaming concept , it will store all the messages of that thread_id in checkpointer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc3293b7-a50c-43c8-a022-8975e1e444b8",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content=\"Los Angeles is currently warmer with a temperature of 21.7°C (71.1°F) compared to San Francisco's 18.9°C (66.0°F).\", response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 1407, 'total_tokens': 1444, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-d2166795-4015-46f5-9abf-899ab09e319b-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0722c3d4-4cbf-43bf-81b0-50f634c4ce61",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [AIMessage(content='Could you please clarify what you are comparing? Are you referring to two specific locations, times, or objects?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 149, 'total_tokens': 172, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o', 'system_fingerprint': 'fp_25624ae3a5', 'finish_reason': 'stop', 'logprobs': None}, id='run-18d7f494-fb14-45e6-bebe-4a49688dde4d-0')]}\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Which one is warmer?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for event in abot.graph.stream({\"messages\": messages}, thread):\n",
    "    for v in event.values():\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace59a36-3941-459e-b9d1-ac5a4a1ed3ae",
   "metadata": {},
   "source": [
    "## Streaming tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5a91e6",
   "metadata": {},
   "source": [
    "But what about streaming tokens themselves.\n",
    "For that, we're going to want to use the A-stream\n",
    "events method that comes on all LangChain and LangGraph objects.\n",
    "A-stream event is an asynchronous method,\n",
    "which means that we're going to need to use an async checkpointer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b2f82fe-3ec4-4917-be51-9fb10d1317fa",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.aiosqlite import AsyncSqliteSaver\n",
    "\n",
    "memory = AsyncSqliteSaver.from_conn_string(\":memory:\")\n",
    "abot = Agent(model, [tool], system=prompt, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee0fe1c7-77e2-499c-a2f9-1f739bb6ddf0",
   "metadata": {
    "height": 200
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/langchain_core/_api/beta_decorator.py:87: LangChainBetaWarning: This API is in beta and may change in the future.\n",
      "  warn_beta(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling: {'name': 'tavily_search_results_json', 'args': {'query': 'current weather in San Francisco'}, 'id': 'call_Ln5Sfh6c9Br3iLLnu3ghaAEE'}\n",
      "Back to the model!\n",
      "The| current| weather| in| San| Francisco| is| partly| cloudy| with| a| temperature| of| |18|.|9|°C| (|66|.|0|°F|).| The| wind| is| blowing| from| the| west|-s|outh|west| at| |8|.|9| mph| (|14|.|4| k|ph|),| and| the| humidity| level| is| at| |78|%.| The| atmospheric| pressure| is| |100|9|.|0| mb|.| Visibility| is| good| at| |16| km| (|9| miles|).|"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "async for event in abot.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        content = event[\"data\"][\"chunk\"].content\n",
    "        if content:\n",
    "            # Empty content in the context of OpenAI means\n",
    "            # that the model is asking for a tool to be invoked.\n",
    "            # So we only print non-empty content\n",
    "            print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f303b1-a4d0-408c-8cc0-515ff980717f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
